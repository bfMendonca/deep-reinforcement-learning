{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reacher solution\n",
    "\n",
    "---\n",
    "\n",
    "This notebook will present and proposal for a solution for the Udacity Reacher environment and will serve as an guideline in order to describe how this solved the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.20 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The cell below act as an abstraction for using one of the many environment variants of the workspace. If the variable **'MULTI_AGENT'** is set by the user, this workspace will understand that we would like to solve the problem using the Unity simulator with 20 reacher arms, otherwise, the single reacher arm would be used. \n",
    "\n",
    "By enabling the visualization, through the **'VIS_ENABLED'**, the GUI of the simulator can be turned on or off.\n",
    "\n",
    "\n",
    "**NOTE**: Bellow we hardcode the location of the files for the reacher environment. Feel free to adapt it as needed in order to make this notebook run with the desired variant of the problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ReacherBrain']\n"
     ]
    }
   ],
   "source": [
    "MULTI_AGENT = True\n",
    "VIS_ENABLED = False\n",
    "\n",
    "if 'MULTI_AGENT' in globals() and MULTI_AGENT:\n",
    "    if 'VIS_ENABLED' in globals() and VIS_ENABLED:\n",
    "        #env = UnityEnvironment(file_name='one_agent_reacher_novis/Reacher.x86_64')\n",
    "        pass\n",
    "        \n",
    "    else:\n",
    "        env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "        \n",
    "        \n",
    "else:\n",
    "    if 'VIS_ENABLED' in globals() and VIS_ENABLED:\n",
    "        #env = UnityEnvironment(file_name='many_agents_reacher_novis/Reacher.x86_64')\n",
    "        pass\n",
    "        \n",
    "    else:\n",
    "        env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "        \n",
    "        \n",
    "    \n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "print( env.brain_names )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model hyperparameters and configuration\n",
    "\n",
    "The cell below uses the DDPG agent for solving the problem and passes as its constructor arguments all the hyperparameters that we exposed and tweaked to achieve the needed performance. Also, for clarity, the DDPG Agent will import and use a PyTorch model for \"actor\" and \"critic\". We will print this network to expose its guts below, but the model is the same presented at the Udacity DDPG Pendulum example but with the addition of a Batch normalization layer in both networks at the first layer.\n",
    "\n",
    "This batch normalization was seen in other implementations of this problem across GitHub and made the training finally become stable and achieve the score needed for submission. \n",
    "\n",
    "For the agent itself, we implemented the mechanism described in the problem statement called **learn_prescaler** which controls how ofter we want the agent to run the learning phase. After the agent has performed \"learn_prescaler_s\" steps it will perform the number of **learning_cycles** consecutively. \n",
    "\n",
    "The **sample_every_cycle** config was a minor test performed in which this author tried to understand if learning **learning_cycles** with the same sample from the replay buffer would improve performance but it had not made a positive effect. The default behavior, 'sample_every_cycle=True', was the best performant and this was maintained for historical purposes. \n",
    "\n",
    "Another approach made to understand how to make the training more stable was to constantly decrease the noise applied to the action during the training phase. This noise, as far I understand it, makes it possible for the agent to explore during training, applying actions that were not predicted by its \"actor\" network and thus, may lead the network convergence process to explore regions of its parameters spaces that maybe would not be easily accessed in any other way. The **noise_initial_gain** and **noise_gain_decay** were introduced to make the noise steadily decreases as the training progressed to decrease how our agent would explore in lather states of the training but this showed no improvement over the default implemented mechanism and, as the 'sample_every_cycle', was maintained for documenting which were and were not tested during the development phase. \n",
    "\n",
    "\n",
    "\n",
    "Finally **batch_normalize** enable or disable a \"batch normalization\" for the first layer of the \"actor\" and \"critic\" network and, in the matter of fact, was the parameter that mostly contributed to achieving the needed performance. This variable allowed to easily \"enable\" and \"disable\" this process to understand if it were actually what was majorly contributing to the improvements seen at the scores during training.\n",
    "\n",
    "## TL;DR\n",
    "\n",
    "**buffer_size**: Total number of steps saved which can be sampled for the learning.\n",
    "\n",
    "**batch_size**: How many steps are actually sampled for each learning phase.\n",
    "\n",
    "**gamma**: Discount factor applied to the rewards.\n",
    "\n",
    "**tau**: Soft update factor for the \"Target\" and \"Local\" variants for the network.\n",
    "\n",
    "**lr_actor**: Learning rate for the \"Actor\" network.\n",
    "\n",
    "**lr_critic**: Learning rate for the \"Critic\" network.\n",
    "\n",
    "**actor_weight_decay**: L2 Weight decay used for the Adam Optmizer of the Actor network.\n",
    "\n",
    "**critic_weight_decay**: L2 Weight decay used for the Adam Optmizer of the Critic network.\n",
    "\n",
    "**learn_prescaler**: How often the agent should run the learn. One each \"learn_prescaler\". 0 disables it and the \n",
    "agent will learn for every step.\n",
    "\n",
    "**learning_cycles**: How many consecutive learns the agent should perform. \n",
    "\n",
    "**noise_initial_gain**: Initial value for a \"gain\" multiplied to the noise added to the actor actions.\n",
    "\n",
    "**noise_gain_decay**: How much 'noise_initial_gain' should decrease for each step.\n",
    "\n",
    "**sample_every_cycle**: If the agent should sample its replay buffer for each consecutive learn\n",
    "\n",
    "**gradient_limiter**: If the critic grandient should be limited before running th backward prop. \n",
    "\n",
    "**batch_normalize**: Enable or disable the batch normalization of the first layer of both networks, agent and critic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import Agent\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "\n",
    "agent = Agent(state_size          = 33, \n",
    "              action_size         = 4, \n",
    "              random_seed         = 2, \n",
    "              buffer_size         = 100000,\n",
    "              batch_size          = 128,\n",
    "              gamma               = 0.99,\n",
    "              tau                 = 1e-3,\n",
    "              lr_actor            = 2e-4,\n",
    "              lr_critic           = 2e-4,\n",
    "              actor_weight_decay  = 0,\n",
    "              critic_weight_decay = 0,\n",
    "              learn_prescaler     = 20, \n",
    "              learning_cycles     = 10, \n",
    "              noise_initial_gain  = 1.0,\n",
    "              noise_gain_decay    = 1.0,\n",
    "              sample_every_cycle  = True, \n",
    "              gradient_limiter    = True,\n",
    "              batch_normalize     = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent PyTorch Network\n",
      "\n",
      "Actor(\n",
      "  (fc1): Linear(in_features=33, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=4, bias=True)\n",
      "  (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "\n",
      "Critic PyTorch Network\n",
      "\n",
      "Critic(\n",
      "  (fcs1): Linear(in_features=33, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=260, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print( \"Agent PyTorch Network\\n\")\n",
    "print( agent.actor_local )\n",
    "\n",
    "\n",
    "print( \"\\n\\nCritic PyTorch Network\\n\")\n",
    "print( agent.critic_local )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workspace/ddpg_agent.py:171: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19\tAverage Score: 26.51"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "def ddpg(n_episodes=150, max_t=10000, print_every=100):   \n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        states = env.reset(train_mode=True)[brain_name].vector_observations     # reset the environment    \n",
    "        agent.reset()\n",
    "        \n",
    "        scores_episode = np.zeros(num_agents)               # rewards per episode for each agent\n",
    "        \n",
    "        for t in range( max_t ):\n",
    "            actions     = agent.act(states)\n",
    "            env_info    = env.step(actions)[brain_name]     # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations      # get next state (for each agent)\n",
    "            rewards     = env_info.rewards                  # get reward (for each agent)\n",
    "            dones       = env_info.local_done               # see if episode finished\n",
    "            \n",
    "            for (state, action, reward, next_state, done) in zip(states, actions, rewards, next_states, dones):\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            \n",
    "            states = next_states\n",
    "            scores_episode += rewards\n",
    "            \n",
    "            if any(dones):\n",
    "                break \n",
    "                \n",
    "        \n",
    "        #Averaring the mean score across all agents for this episode\n",
    "        mean_score = np.mean( scores_episode )\n",
    "\n",
    "        scores.append( mean_score )\n",
    "        scores_deque.append( mean_score )\n",
    "        \n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean( scores_deque )), end=\"\")\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean( scores_deque )))\n",
    "            \n",
    "    return scores\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions     = agent.act(states)                    # select an action (for each agent)\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
